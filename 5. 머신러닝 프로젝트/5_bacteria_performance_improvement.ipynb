{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.datasets import load_digits\n",
    "from matplotlib import font_manager\n",
    "from matplotlib import gridspec\n",
    "from math import factorial\n",
    "import sklearn\n",
    "import pprint\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "font_fname = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_family = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "\n",
    "plt.rcParams[\"font.family\"] = font_family\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train_transform_weight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_weight = pd.read_csv('./train_transform.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_df['target'] = label_encoder.fit_transform(train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['gcd', 'target', 'sample_weight'], axis=1)\n",
    "y = train_df['target']\n",
    "sample_weight = train_df['sample_weight']\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "train_weight = sample_weight.iloc[X_train.index]\n",
    "test_weight = sample_weight.iloc[X_test.index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초기 설정값 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTreesClassifier 과 LGBMClassifier을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9452475470002247"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_clf = ExtraTreesClassifier(n_estimators=300, n_jobs=-1)\n",
    "et_clf.fit(X_train, y_train, train_weight)\n",
    "pred = et_clf.predict(X_test)\n",
    "score = accuracy_score(y_test, pred, sample_weight=test_weight)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_search(X, y, sample_weight, model, params=None, scoring='accuracy', cv=5):\n",
    "    \n",
    "    grid_clf = GridSearchCV(model, param_grid=params, scoring=scoring, cv=cv)\n",
    "    grid_clf.fit(X, y, sample_weight=sample_weight)\n",
    "    \n",
    "    print(grid_clf.best_params_, grid_clf.best_score_)\n",
    "    \n",
    "    scores_df = pd.DataFrame(grid_clf.cv_results_)\n",
    "    return scores_df[['params', 'mean_test_score', 'rank_test_score',\n",
    "                      'split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "{'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 4} 0.9626388708974165\n",
      "LGBMClassifier\n",
      "{'max_depth': 45, 'min_child_samples': 12, 'reg_alpha': 1} 0.9653809567868199\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    ExtraTreesClassifier(warm_start=True, n_estimators=300, n_jobs=-1, random_state=0): \n",
    "    {'max_depth': [45, 50, 55],\n",
    "    'min_samples_split':[4, 8, 12],\n",
    "    'min_samples_leaf': [4, 8, 12]\n",
    "},\n",
    "    LGBMClassifier(n_estimators=300, n_jobs=-1, random_state=0): \n",
    "    {'max_depth': [45, 50, 55],\n",
    "    'min_child_samples':[4, 8, 12],\n",
    "    'reg_alpha': [1, 3, 5]\n",
    "}\n",
    "}\n",
    "scores_df_list = []\n",
    "for model, params in models.items():\n",
    "    print(type(model).__name__)\n",
    "    scores_df = best_search(X_train, y_train, train_weight, model, params=params)\n",
    "    scores_df_list.append(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 4}</td>\n",
       "      <td>0.962639</td>\n",
       "      <td>1</td>\n",
       "      <td>0.960684</td>\n",
       "      <td>0.963305</td>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.962750</td>\n",
       "      <td>0.962799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 8}</td>\n",
       "      <td>0.962639</td>\n",
       "      <td>1</td>\n",
       "      <td>0.960684</td>\n",
       "      <td>0.963305</td>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.962750</td>\n",
       "      <td>0.962799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 45, 'min_samples_leaf': 4, 'min_samples_split': 4}</td>\n",
       "      <td>0.962568</td>\n",
       "      <td>3</td>\n",
       "      <td>0.960885</td>\n",
       "      <td>0.962296</td>\n",
       "      <td>0.963758</td>\n",
       "      <td>0.962649</td>\n",
       "      <td>0.963252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 45, 'min_samples_leaf': 4, 'min_samples_split': 8}</td>\n",
       "      <td>0.962568</td>\n",
       "      <td>3</td>\n",
       "      <td>0.960885</td>\n",
       "      <td>0.962296</td>\n",
       "      <td>0.963758</td>\n",
       "      <td>0.962649</td>\n",
       "      <td>0.963252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'max_depth': 55, 'min_samples_leaf': 4, 'min_samples_split': 8}</td>\n",
       "      <td>0.962467</td>\n",
       "      <td>5</td>\n",
       "      <td>0.960633</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.964162</td>\n",
       "      <td>0.962851</td>\n",
       "      <td>0.962950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              params  \\\n",
       "9   {'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 4}   \n",
       "10  {'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 8}   \n",
       "0   {'max_depth': 45, 'min_samples_leaf': 4, 'min_samples_split': 4}   \n",
       "1   {'max_depth': 45, 'min_samples_leaf': 4, 'min_samples_split': 8}   \n",
       "19  {'max_depth': 55, 'min_samples_leaf': 4, 'min_samples_split': 8}   \n",
       "\n",
       "    mean_test_score  rank_test_score  split0_test_score  split1_test_score  \\\n",
       "9          0.962639                1           0.960684           0.963305   \n",
       "10         0.962639                1           0.960684           0.963305   \n",
       "0          0.962568                3           0.960885           0.962296   \n",
       "1          0.962568                3           0.960885           0.962296   \n",
       "19         0.962467                5           0.960633           0.961742   \n",
       "\n",
       "    split2_test_score  split3_test_score  split4_test_score  \n",
       "9            0.963657           0.962750           0.962799  \n",
       "10           0.963657           0.962750           0.962799  \n",
       "0            0.963758           0.962649           0.963252  \n",
       "1            0.963758           0.962649           0.963252  \n",
       "19           0.964162           0.962851           0.962950  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df_list[0].sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'max_depth': 55, 'min_child_samples': 12, 'reg_alpha': 1}</td>\n",
       "      <td>0.965381</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963305</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.966833</td>\n",
       "      <td>0.966833</td>\n",
       "      <td>0.964008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'max_depth': 45, 'min_child_samples': 12, 'reg_alpha': 1}</td>\n",
       "      <td>0.965381</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963305</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.966833</td>\n",
       "      <td>0.966833</td>\n",
       "      <td>0.964008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'max_depth': 50, 'min_child_samples': 12, 'reg_alpha': 1}</td>\n",
       "      <td>0.965381</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963305</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.966833</td>\n",
       "      <td>0.966833</td>\n",
       "      <td>0.964008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 45, 'min_child_samples': 8, 'reg_alpha': 1}</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>4</td>\n",
       "      <td>0.963204</td>\n",
       "      <td>0.965825</td>\n",
       "      <td>0.967186</td>\n",
       "      <td>0.966279</td>\n",
       "      <td>0.963555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'max_depth': 55, 'min_child_samples': 8, 'reg_alpha': 1}</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>4</td>\n",
       "      <td>0.963204</td>\n",
       "      <td>0.965825</td>\n",
       "      <td>0.967186</td>\n",
       "      <td>0.966279</td>\n",
       "      <td>0.963555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        params  \\\n",
       "24  {'max_depth': 55, 'min_child_samples': 12, 'reg_alpha': 1}   \n",
       "6   {'max_depth': 45, 'min_child_samples': 12, 'reg_alpha': 1}   \n",
       "15  {'max_depth': 50, 'min_child_samples': 12, 'reg_alpha': 1}   \n",
       "3    {'max_depth': 45, 'min_child_samples': 8, 'reg_alpha': 1}   \n",
       "21   {'max_depth': 55, 'min_child_samples': 8, 'reg_alpha': 1}   \n",
       "\n",
       "    mean_test_score  rank_test_score  split0_test_score  split1_test_score  \\\n",
       "24         0.965381                1           0.963305           0.965926   \n",
       "6          0.965381                1           0.963305           0.965926   \n",
       "15         0.965381                1           0.963305           0.965926   \n",
       "3          0.965210                4           0.963204           0.965825   \n",
       "21         0.965210                4           0.963204           0.965825   \n",
       "\n",
       "    split2_test_score  split3_test_score  split4_test_score  \n",
       "24           0.966833           0.966833           0.964008  \n",
       "6            0.966833           0.966833           0.964008  \n",
       "15           0.966833           0.966833           0.964008  \n",
       "3            0.967186           0.966279           0.963555  \n",
       "21           0.967186           0.966279           0.963555  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df_list[1].sort_values('rank_test_score').head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTreesClassifier\n",
    "{'max_depth': 52, 'min_samples_leaf': 2, 'min_samples_split': 2} 0.9628808222425708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "{'max_depth': 52, 'min_samples_leaf': 1, 'min_samples_split': 2} 0.9678206273883123\n",
      "LGBMClassifier\n",
      "{'max_depth': 25, 'min_child_samples': 20, 'reg_alpha': 0.3} 0.9655422603211015\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    ExtraTreesClassifier(n_estimators=300, n_jobs=-1, random_state=0): \n",
    "    {'max_depth': [50, 52, 54],\n",
    "    'min_samples_split':[1, 2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "},\n",
    "    LGBMClassifier(n_estimators=300, n_jobs=-1, random_state=0): \n",
    "    {'max_depth': [25, 35, 45],\n",
    "    'min_child_samples':[12, 16, 20],\n",
    "    'reg_alpha': [0.3, 0.6, 1]\n",
    "}}\n",
    "\n",
    "scores_df_list = []\n",
    "for model, params in models.items():\n",
    "    print(type(model).__name__)\n",
    "    scores_df = best_search(X_train, y_train, train_weight, model, params=params)\n",
    "    scores_df_list.append(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'max_depth': 52, 'min_samples_leaf': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.967821</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965522</td>\n",
       "      <td>0.969152</td>\n",
       "      <td>0.968698</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>0.967134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.967458</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965472</td>\n",
       "      <td>0.967186</td>\n",
       "      <td>0.968043</td>\n",
       "      <td>0.968446</td>\n",
       "      <td>0.968142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.967448</td>\n",
       "      <td>3</td>\n",
       "      <td>0.965371</td>\n",
       "      <td>0.968043</td>\n",
       "      <td>0.967740</td>\n",
       "      <td>0.968900</td>\n",
       "      <td>0.967184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'max_depth': 54, 'min_samples_leaf': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.967337</td>\n",
       "      <td>4</td>\n",
       "      <td>0.965674</td>\n",
       "      <td>0.967337</td>\n",
       "      <td>0.968698</td>\n",
       "      <td>0.968648</td>\n",
       "      <td>0.966327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'max_depth': 52, 'min_samples_leaf': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.967155</td>\n",
       "      <td>5</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>0.968244</td>\n",
       "      <td>0.967387</td>\n",
       "      <td>0.968093</td>\n",
       "      <td>0.966428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              params  \\\n",
       "10  {'max_depth': 52, 'min_samples_leaf': 1, 'min_samples_split': 2}   \n",
       "2   {'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 3}   \n",
       "1   {'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 2}   \n",
       "20  {'max_depth': 54, 'min_samples_leaf': 1, 'min_samples_split': 3}   \n",
       "11  {'max_depth': 52, 'min_samples_leaf': 1, 'min_samples_split': 3}   \n",
       "\n",
       "    mean_test_score  rank_test_score  split0_test_score  split1_test_score  \\\n",
       "10         0.967821                1           0.965522           0.969152   \n",
       "2          0.967458                2           0.965472           0.967186   \n",
       "1          0.967448                3           0.965371           0.968043   \n",
       "20         0.967337                4           0.965674           0.967337   \n",
       "11         0.967155                5           0.965623           0.968244   \n",
       "\n",
       "    split2_test_score  split3_test_score  split4_test_score  \n",
       "10           0.968698           0.968597           0.967134  \n",
       "2            0.968043           0.968446           0.968142  \n",
       "1            0.967740           0.968900           0.967184  \n",
       "20           0.968698           0.968648           0.966327  \n",
       "11           0.967387           0.968093           0.966428  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df_list[0].sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'max_depth': 35, 'min_child_samples': 20, 'reg_alpha': 0.3}</td>\n",
       "      <td>0.965542</td>\n",
       "      <td>1</td>\n",
       "      <td>0.964363</td>\n",
       "      <td>0.965018</td>\n",
       "      <td>0.966379</td>\n",
       "      <td>0.967438</td>\n",
       "      <td>0.964513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'max_depth': 45, 'min_child_samples': 20, 'reg_alpha': 0.3}</td>\n",
       "      <td>0.965542</td>\n",
       "      <td>1</td>\n",
       "      <td>0.964363</td>\n",
       "      <td>0.965018</td>\n",
       "      <td>0.966379</td>\n",
       "      <td>0.967438</td>\n",
       "      <td>0.964513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'max_depth': 25, 'min_child_samples': 20, 'reg_alpha': 0.3}</td>\n",
       "      <td>0.965542</td>\n",
       "      <td>1</td>\n",
       "      <td>0.964363</td>\n",
       "      <td>0.965018</td>\n",
       "      <td>0.966379</td>\n",
       "      <td>0.967438</td>\n",
       "      <td>0.964513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 25, 'min_child_samples': 16, 'reg_alpha': 0.3}</td>\n",
       "      <td>0.965542</td>\n",
       "      <td>4</td>\n",
       "      <td>0.964162</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.967186</td>\n",
       "      <td>0.966379</td>\n",
       "      <td>0.964210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'max_depth': 45, 'min_child_samples': 16, 'reg_alpha': 0.3}</td>\n",
       "      <td>0.965542</td>\n",
       "      <td>4</td>\n",
       "      <td>0.964162</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.967186</td>\n",
       "      <td>0.966379</td>\n",
       "      <td>0.964210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          params  \\\n",
       "15  {'max_depth': 35, 'min_child_samples': 20, 'reg_alpha': 0.3}   \n",
       "24  {'max_depth': 45, 'min_child_samples': 20, 'reg_alpha': 0.3}   \n",
       "6   {'max_depth': 25, 'min_child_samples': 20, 'reg_alpha': 0.3}   \n",
       "3   {'max_depth': 25, 'min_child_samples': 16, 'reg_alpha': 0.3}   \n",
       "21  {'max_depth': 45, 'min_child_samples': 16, 'reg_alpha': 0.3}   \n",
       "\n",
       "    mean_test_score  rank_test_score  split0_test_score  split1_test_score  \\\n",
       "15         0.965542                1           0.964363           0.965018   \n",
       "24         0.965542                1           0.964363           0.965018   \n",
       "6          0.965542                1           0.964363           0.965018   \n",
       "3          0.965542                4           0.964162           0.965774   \n",
       "21         0.965542                4           0.964162           0.965774   \n",
       "\n",
       "    split2_test_score  split3_test_score  split4_test_score  \n",
       "15           0.966379           0.967438           0.964513  \n",
       "24           0.966379           0.967438           0.964513  \n",
       "6            0.966379           0.967438           0.964513  \n",
       "3            0.967186           0.966379           0.964210  \n",
       "21           0.967186           0.966379           0.964210  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df_list[1].sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "{'max_depth': 52, 'n_estimators': 600} 0.9689094051554982\n",
      "LGBMClassifier\n",
      "{'max_depth': 25, 'min_child_samples': 20, 'reg_alpha': 0.2} 0.9658951072922995\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    ExtraTreesClassifier(warm_start=True, n_jobs=-1,max_depth=52, random_state=0): \n",
    "    {'max_depth': [52],\n",
    "    'n_estimators': [300, 400, 500, 600]\n",
    "},\n",
    "    LGBMClassifier(n_estimators=300, n_jobs=-1, random_state=0): \n",
    "    {'max_depth': [15, 25],\n",
    "    'min_child_samples':[20, 30, 40],\n",
    "    'reg_alpha': [0.2]\n",
    "}}\n",
    "\n",
    "scores_df_list = []\n",
    "for model, params in models.items():\n",
    "    print(type(model).__name__)\n",
    "    scores_df = best_search(X_train, y_train, train_weight, model, params=params)\n",
    "    scores_df_list.append(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 52, 'n_estimators': 600}</td>\n",
       "      <td>0.968909</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966228</td>\n",
       "      <td>0.970311</td>\n",
       "      <td>0.969152</td>\n",
       "      <td>0.970412</td>\n",
       "      <td>0.968444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 52, 'n_estimators': 500}</td>\n",
       "      <td>0.968647</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965976</td>\n",
       "      <td>0.970109</td>\n",
       "      <td>0.969706</td>\n",
       "      <td>0.969505</td>\n",
       "      <td>0.967940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 52, 'n_estimators': 400}</td>\n",
       "      <td>0.968446</td>\n",
       "      <td>3</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>0.970160</td>\n",
       "      <td>0.969152</td>\n",
       "      <td>0.969404</td>\n",
       "      <td>0.967890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 52, 'n_estimators': 300}</td>\n",
       "      <td>0.967821</td>\n",
       "      <td>4</td>\n",
       "      <td>0.965522</td>\n",
       "      <td>0.969152</td>\n",
       "      <td>0.968698</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>0.967134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   params  mean_test_score  rank_test_score  \\\n",
       "3  {'max_depth': 52, 'n_estimators': 600}         0.968909                1   \n",
       "2  {'max_depth': 52, 'n_estimators': 500}         0.968647                2   \n",
       "1  {'max_depth': 52, 'n_estimators': 400}         0.968446                3   \n",
       "0  {'max_depth': 52, 'n_estimators': 300}         0.967821                4   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "3           0.966228           0.970311           0.969152           0.970412   \n",
       "2           0.965976           0.970109           0.969706           0.969505   \n",
       "1           0.965623           0.970160           0.969152           0.969404   \n",
       "0           0.965522           0.969152           0.968698           0.968597   \n",
       "\n",
       "   split4_test_score  \n",
       "3           0.968444  \n",
       "2           0.967940  \n",
       "1           0.967890  \n",
       "0           0.967134  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df_list[0].sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'max_depth': 25, 'min_child_samples': 20, 'reg_alpha': 0.2}</td>\n",
       "      <td>0.965895</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963557</td>\n",
       "      <td>0.966127</td>\n",
       "      <td>0.967539</td>\n",
       "      <td>0.967085</td>\n",
       "      <td>0.965168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 15, 'min_child_samples': 30, 'reg_alpha': 0}</td>\n",
       "      <td>0.965895</td>\n",
       "      <td>2</td>\n",
       "      <td>0.964212</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.967287</td>\n",
       "      <td>0.967942</td>\n",
       "      <td>0.964261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 15, 'min_child_samples': 40, 'reg_alpha': 0}</td>\n",
       "      <td>0.965835</td>\n",
       "      <td>3</td>\n",
       "      <td>0.964514</td>\n",
       "      <td>0.965270</td>\n",
       "      <td>0.967035</td>\n",
       "      <td>0.967287</td>\n",
       "      <td>0.965067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'max_depth': 25, 'min_child_samples': 30, 'reg_alpha': 0.2}</td>\n",
       "      <td>0.965835</td>\n",
       "      <td>4</td>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.967589</td>\n",
       "      <td>0.966682</td>\n",
       "      <td>0.964765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 15, 'min_child_samples': 40, 'reg_alpha': 0.2}</td>\n",
       "      <td>0.965754</td>\n",
       "      <td>5</td>\n",
       "      <td>0.963456</td>\n",
       "      <td>0.966027</td>\n",
       "      <td>0.967337</td>\n",
       "      <td>0.967085</td>\n",
       "      <td>0.964865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         params  \\\n",
       "7  {'max_depth': 25, 'min_child_samples': 20, 'reg_alpha': 0.2}   \n",
       "2    {'max_depth': 15, 'min_child_samples': 30, 'reg_alpha': 0}   \n",
       "4    {'max_depth': 15, 'min_child_samples': 40, 'reg_alpha': 0}   \n",
       "9  {'max_depth': 25, 'min_child_samples': 30, 'reg_alpha': 0.2}   \n",
       "5  {'max_depth': 15, 'min_child_samples': 40, 'reg_alpha': 0.2}   \n",
       "\n",
       "   mean_test_score  rank_test_score  split0_test_score  split1_test_score  \\\n",
       "7         0.965895                1           0.963557           0.966127   \n",
       "2         0.965895                2           0.964212           0.965774   \n",
       "4         0.965835                3           0.964514           0.965270   \n",
       "9         0.965835                4           0.963657           0.966480   \n",
       "5         0.965754                5           0.963456           0.966027   \n",
       "\n",
       "   split2_test_score  split3_test_score  split4_test_score  \n",
       "7           0.967539           0.967085           0.965168  \n",
       "2           0.967287           0.967942           0.964261  \n",
       "4           0.967035           0.967287           0.965067  \n",
       "9           0.967589           0.966682           0.964765  \n",
       "5           0.967337           0.967085           0.964865  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df_list[1].sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier\n",
      "{'learning_rate': 0.1} 0.9674173776997378\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    LGBMClassifier(n_estimators=1000,n_jobs=-1, max_depth=25, min_child_samples=20, reg_alpha=0.2,  random_state=0): \n",
    "    {\n",
    "    'learning_rate': [0.05, 0.07, 0.1]\n",
    "}}\n",
    "\n",
    "scores_df_list = []\n",
    "for model, params in models.items():\n",
    "    print(type(model).__name__)\n",
    "    scores_df = best_search(X_train, y_train, train_weight, model, params=params)\n",
    "    scores_df_list.append(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "{'n_estimators': 2500} 0.970663545645684\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    ExtraTreesClassifier(warm_start=True, n_jobs=-1, max_depth=52, random_state=0): \n",
    "    {\n",
    "    'n_estimators': [500, 1000, 1500, 2000, 2500]\n",
    "}}\n",
    "\n",
    "scores_df_list = []\n",
    "for model, params in models.items():\n",
    "    print(type(model).__name__)\n",
    "    scores_df = best_search(X_train, y_train, train_weight, model, params=params)\n",
    "    scores_df_list.append(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pred_proba(df, model, time=True, splits=10, weight=True):\n",
    "    #%%time\n",
    "    \n",
    "    X = df.drop(['gcd', 'target', 'sample_weight'], axis=1)\n",
    "    y = df['target']\n",
    "    sample_weight = df['sample_weight']\n",
    "\n",
    "    print(type(model).__name__)\n",
    "    \n",
    "    N_SPLITS = splits\n",
    "    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    scores, y_pred_list, y_proba_list = [], [], []\n",
    "    \n",
    "    if time == True:\n",
    "        folds_split = tqdm(folds.split(X, y), total=N_SPLITS)\n",
    "    else:\n",
    "        folds_split = folds.split(X, y)\n",
    "    \n",
    "    for fold, (train_id, valid_id) in enumerate(folds_split):\n",
    "        if time == True:\n",
    "            print('####### Fold: ', fold)\n",
    "        \n",
    "        # Splitting\n",
    "        X_train, y_train, sample_weight_train = X.iloc[train_id], y.iloc[train_id], sample_weight.iloc[train_id]\n",
    "        X_valid, y_valid, sample_weight_valid = X.iloc[valid_id], y.iloc[valid_id], sample_weight.iloc[valid_id]\n",
    "\n",
    "        # Training\n",
    "        if weight == True:\n",
    "            model.fit(X_train, y_train, sample_weight_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # Validation\n",
    "        valid_pred = model.predict(X_valid)\n",
    "        valid_pred_proba = model.predict_proba(X_valid)\n",
    "        valid_score = accuracy_score(y_valid, valid_pred, sample_weight=sample_weight_valid)\n",
    "        \n",
    "        print(f'Accuracy score: {valid_score:5f}\\n')\n",
    "        scores.append(valid_score)\n",
    "        \n",
    "        y_pred_list.append(valid_pred)\n",
    "        y_proba_list.append(valid_pred_proba)\n",
    "\n",
    "\n",
    "    score = np.array(scores).mean()\n",
    "    print(f'Mean accuracy score: {score:6f}')\n",
    "    return y_pred_list, y_proba_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Fold:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:12<01:55, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.951551\n",
      "\n",
      "####### Fold:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:24<01:37, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.947480\n",
      "\n",
      "####### Fold:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:36<01:23, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.952381\n",
      "\n",
      "####### Fold:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:48<01:11, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.948925\n",
      "\n",
      "####### Fold:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:00<00:59, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.945633\n",
      "\n",
      "####### Fold:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:12<00:47, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.951164\n",
      "\n",
      "####### Fold:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:24<00:36, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.945069\n",
      "\n",
      "####### Fold:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:36<00:24, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.945765\n",
      "\n",
      "####### Fold:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:48<00:12, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.946958\n",
      "\n",
      "####### Fold:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:00<00:00, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.953855\n",
      "\n",
      "Mean accuracy score: 0.948878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=600, n_jobs=-1,max_depth=52, random_state=0)\n",
    "y_pred_list, y_proba_list = model_pred_proba(train_df, model, splits=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds, weight=pd.Series(), n_random_state=None):\n",
    "    \n",
    "    if n_random_state != None:\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=n_random_state)\n",
    "    else:\n",
    "        kf = KFold(n_splits=n_folds, shuffle=False)\n",
    "        \n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    \n",
    "    for folder_counter, (train_index, valide_index) in enumerate(kf.split(X_train_n)):\n",
    "        X_tr = X_train_n.iloc[train_index]\n",
    "        y_tr = y_train_n.iloc[train_index]\n",
    "        X_te = X_train_n.iloc[valide_index]\n",
    "        \n",
    "        if weight.shape[0] != 0:\n",
    "            X_tr_weight = weight.iloc[train_index]\n",
    "            model.fit(X_tr, y_tr, sample_weight=X_tr_weight)\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "            \n",
    "        train_fold_pred[valide_index, :] = model.predict(X_te).reshape(-1, 1)\n",
    "        \n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "    \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return train_fold_pred, test_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf = LGBMClassifier(n_estimators=2500,n_jobs=-1, max_depth=25, min_child_samples=20, reg_alpha=0.2,  random_state=0)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=2500, n_jobs=-1, max_depth=52, random_state=0)\n",
    "knn_clf = KNeighborsClassifier(metric='manhattan', n_jobs=-1, weights='distance', n_neighbors=2)\n",
    "lr_final = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_train, lgbm_test 저장\n",
    "np.save('lgbm_train.npy', lgbm_train)\n",
    "np.save('lgbm_test.npy', lgbm_test)\n",
    "\n",
    "# et_train, et_test 저장\n",
    "np.save('et_train.npy', et_train)\n",
    "np.save('et_test.npy', et_test)\n",
    "\n",
    "# knn_train, knn_test 저장\n",
    "np.save('knn_train.npy', knn_train)\n",
    "np.save('knn_test.npy', knn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_train, lgbm_test 불러오기\n",
    "lgbm_train = np.load('lgbm_train.npy')\n",
    "lgbm_test = np.load('lgbm_test.npy')\n",
    "\n",
    "# et_train, et_test 불러오기\n",
    "et_train = np.load('et_train.npy')\n",
    "et_test = np.load('et_test.npy')\n",
    "\n",
    "# knn_train, knn_test 불러오기\n",
    "knn_train = np.load('knn_train.npy')\n",
    "knn_test = np.load('knn_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.9558853179563692\n"
     ]
    }
   ],
   "source": [
    "stack_final_X_train = np.concatenate((lgbm_train, et_train, knn_train), axis=1)\n",
    "stack_final_X_test= np.concatenate((lgbm_test, et_test, knn_test), axis=1)\n",
    "\n",
    "lr_final.fit(stack_final_X_train, y_train)\n",
    "stack_final = lr_final.predict(stack_final_X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, stack_final)\n",
    "print('정확도:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스테킹 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_no_weight_search(X, y, model, params=None, scoring='accuracy', cv=5):\n",
    "    \n",
    "    grid_clf = GridSearchCV(model, param_grid=params, n_jobs=-1, scoring=scoring, cv=cv)\n",
    "    grid_clf.fit(X, y)\n",
    "    \n",
    "    print(grid_clf.best_params_, grid_clf.best_score_)\n",
    "    \n",
    "    scores_df = pd.DataFrame(grid_clf.cv_results_)\n",
    "    return scores_df[['params', 'mean_test_score', 'rank_test_score',\n",
    "                      'split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'} 0.9596749721943774\n"
     ]
    }
   ],
   "source": [
    "params = {'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "          'penalty': ['l2','l1'],\n",
    "          'C': [0.01, 0.1, 1, 5, 10]\n",
    "          }\n",
    "\n",
    "lr_final = LogisticRegression(multi_class='multinomial', n_jobs=-1)\n",
    "\n",
    "scores_df = best_no_weight_search(stack_final_X_train, y_train, lr_final, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n",
       "      <td>0.959675</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959121</td>\n",
       "      <td>0.957911</td>\n",
       "      <td>0.963002</td>\n",
       "      <td>0.959474</td>\n",
       "      <td>0.958867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n",
       "      <td>0.959050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.957357</td>\n",
       "      <td>0.957609</td>\n",
       "      <td>0.960179</td>\n",
       "      <td>0.961188</td>\n",
       "      <td>0.958917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}</td>\n",
       "      <td>0.958616</td>\n",
       "      <td>3</td>\n",
       "      <td>0.957054</td>\n",
       "      <td>0.959272</td>\n",
       "      <td>0.959776</td>\n",
       "      <td>0.958718</td>\n",
       "      <td>0.958262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}</td>\n",
       "      <td>0.958526</td>\n",
       "      <td>4</td>\n",
       "      <td>0.957004</td>\n",
       "      <td>0.959272</td>\n",
       "      <td>0.959373</td>\n",
       "      <td>0.958718</td>\n",
       "      <td>0.958262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'C': 10, 'penalty': 'l1', 'solver': 'saga'}</td>\n",
       "      <td>0.958445</td>\n",
       "      <td>5</td>\n",
       "      <td>0.956601</td>\n",
       "      <td>0.959272</td>\n",
       "      <td>0.959373</td>\n",
       "      <td>0.958718</td>\n",
       "      <td>0.958262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            params  mean_test_score  \\\n",
       "24    {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}         0.959675   \n",
       "16    {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}         0.959050   \n",
       "7   {'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}         0.958616   \n",
       "15   {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}         0.958526   \n",
       "39    {'C': 10, 'penalty': 'l1', 'solver': 'saga'}         0.958445   \n",
       "\n",
       "    rank_test_score  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "24                1           0.959121           0.957911           0.963002   \n",
       "16                2           0.957357           0.957609           0.960179   \n",
       "7                 3           0.957054           0.959272           0.959776   \n",
       "15                4           0.957004           0.959272           0.959373   \n",
       "39                5           0.956601           0.959272           0.959373   \n",
       "\n",
       "    split3_test_score  split4_test_score  \n",
       "24           0.959474           0.958867  \n",
       "16           0.961188           0.958917  \n",
       "7            0.958718           0.958262  \n",
       "15           0.958718           0.958262  \n",
       "39           0.958718           0.958262  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.sort_values(by='rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final = LogisticRegression(C=5, penalty='l2',multi_class='multinomial', solver='lbfgs', n_jobs=-1)\n",
    "\n",
    "accuracy_list3 = {}\n",
    "for lgbm_weight in [0.5, 1, 1.5]:\n",
    "    for et_weight in [0.5, 1, 1.5]:\n",
    "        for knn_weight in [0.3, 0.6, 0.9]:\n",
    "            stack_final_X_train = np.concatenate((lgbm_train*lgbm_weight, et_train*et_weight, knn_train*knn_weight), axis=1)\n",
    "            stack_final_X_test= np.concatenate((lgbm_test*lgbm_weight, et_test*et_weight, knn_test*knn_weight), axis=1)\n",
    "\n",
    "            lr_final.fit(stack_final_X_train, y_train)\n",
    "            stack_final = lr_final.predict(stack_final_X_test)\n",
    "\n",
    "            accuracy = accuracy_score(y_test, stack_final)\n",
    "            accuracy_list3[f'lgbm_weight: {lgbm_weight}, et_weight: {et_weight}, knn_weight: {knn_weight}'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgbm_weight: 1, et_weight: 1, knn_weight: 0.3': 0.9584660671801283,\n",
       " 'lgbm_weight: 1.5, et_weight: 1.5, knn_weight: 0.3': 0.9583047703536433,\n",
       " 'lgbm_weight: 1, et_weight: 1.5, knn_weight: 0.3': 0.9582241219404009,\n",
       " 'lgbm_weight: 0.5, et_weight: 1.5, knn_weight: 0.3': 0.9581434735271583,\n",
       " 'lgbm_weight: 0.5, et_weight: 1, knn_weight: 0.3': 0.9570950441550062,\n",
       " 'lgbm_weight: 0.5, et_weight: 1, knn_weight: 0.6': 0.9569740715351426,\n",
       " 'lgbm_weight: 1, et_weight: 1, knn_weight: 0.9': 0.9566111536755514,\n",
       " 'lgbm_weight: 1.5, et_weight: 1.5, knn_weight: 0.9': 0.9566111536755514,\n",
       " 'lgbm_weight: 0.5, et_weight: 1.5, knn_weight: 0.9': 0.9564901810556877,\n",
       " 'lgbm_weight: 1.5, et_weight: 1, knn_weight: 0.3': 0.9563288842292028,\n",
       " 'lgbm_weight: 1.5, et_weight: 1.5, knn_weight: 0.6': 0.9560869389894754,\n",
       " 'lgbm_weight: 1, et_weight: 1.5, knn_weight: 0.9': 0.9557643453365054,\n",
       " 'lgbm_weight: 0.5, et_weight: 1.5, knn_weight: 0.6': 0.9557240211298843,\n",
       " 'lgbm_weight: 1.5, et_weight: 1, knn_weight: 0.6': 0.9556030485100205,\n",
       " 'lgbm_weight: 1.5, et_weight: 0.5, knn_weight: 0.3': 0.9554417516835356,\n",
       " 'lgbm_weight: 0.5, et_weight: 0.5, knn_weight: 0.9': 0.9553611032702931,\n",
       " 'lgbm_weight: 0.5, et_weight: 1, knn_weight: 0.9': 0.955320779063672,\n",
       " 'lgbm_weight: 1, et_weight: 1, knn_weight: 0.6': 0.9551191580305658,\n",
       " 'lgbm_weight: 1, et_weight: 0.5, knn_weight: 0.6': 0.9550385096173233,\n",
       " 'lgbm_weight: 0.5, et_weight: 0.5, knn_weight: 0.6': 0.9547562401709746,\n",
       " 'lgbm_weight: 1.5, et_weight: 1, knn_weight: 0.9': 0.9546755917577322,\n",
       " 'lgbm_weight: 0.5, et_weight: 0.5, knn_weight: 0.3': 0.9545546191378684,\n",
       " 'lgbm_weight: 1, et_weight: 0.5, knn_weight: 0.3': 0.9538691076253075,\n",
       " 'lgbm_weight: 1.5, et_weight: 0.5, knn_weight: 0.6': 0.9533448929392314,\n",
       " 'lgbm_weight: 1.5, et_weight: 0.5, knn_weight: 0.9': 0.9522964635670793,\n",
       " 'lgbm_weight: 1, et_weight: 0.5, knn_weight: 0.9': 0.9522158151538368,\n",
       " 'lgbm_weight: 1, et_weight: 1.5, knn_weight: 0.6': 0.868744707447881}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data = sorted(accuracy_list3.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_dict = {k: v for k, v in sorted_data}\n",
    " \n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final = LogisticRegression(C=5, penalty='l2',multi_class='multinomial', solver='lbfgs', n_jobs=-1)\n",
    "\n",
    "accuracy_list3 = {}\n",
    "for lgbm_weight in [0.8, 1, 1.2]:\n",
    "    for et_weight in [0.8, 1, 1.2]:\n",
    "        for knn_weight in [0.05, 0.1, 0.15]:\n",
    "            stack_final_X_train = np.concatenate((lgbm_train*lgbm_weight, et_train*et_weight, knn_train*knn_weight), axis=1)\n",
    "            stack_final_X_test= np.concatenate((lgbm_test*lgbm_weight, et_test*et_weight, knn_test*knn_weight), axis=1)\n",
    "\n",
    "            lr_final.fit(stack_final_X_train, y_train)\n",
    "            stack_final = lr_final.predict(stack_final_X_test)\n",
    "\n",
    "            accuracy = accuracy_score(y_test, stack_final)\n",
    "            accuracy_list3[f'lgbm_weight: {lgbm_weight}, et_weight: {et_weight}, knn_weight: {knn_weight}'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgbm_weight: 1.2, et_weight: 1.2, knn_weight: 0.15': 0.9600387112383564,\n",
       " 'lgbm_weight: 1.2, et_weight: 1.2, knn_weight: 0.05': 0.9597161175853866,\n",
       " 'lgbm_weight: 1.2, et_weight: 1.2, knn_weight: 0.1': 0.9597161175853866,\n",
       " 'lgbm_weight: 1, et_weight: 1.2, knn_weight: 0.1': 0.959635469172144,\n",
       " 'lgbm_weight: 1, et_weight: 0.8, knn_weight: 0.05': 0.9595144965522804,\n",
       " 'lgbm_weight: 0.8, et_weight: 1.2, knn_weight: 0.1': 0.9593935239324166,\n",
       " 'lgbm_weight: 0.8, et_weight: 1.2, knn_weight: 0.15': 0.9592322271059317,\n",
       " 'lgbm_weight: 1.2, et_weight: 1, knn_weight: 0.15': 0.9590709302794468,\n",
       " 'lgbm_weight: 1, et_weight: 0.8, knn_weight: 0.15': 0.9589902818662043,\n",
       " 'lgbm_weight: 1, et_weight: 1, knn_weight: 0.05': 0.958949957659583,\n",
       " 'lgbm_weight: 1, et_weight: 1.2, knn_weight: 0.15': 0.958949957659583,\n",
       " 'lgbm_weight: 0.8, et_weight: 1, knn_weight: 0.05': 0.9588289850397194,\n",
       " 'lgbm_weight: 0.8, et_weight: 0.8, knn_weight: 0.05': 0.9587886608330981,\n",
       " 'lgbm_weight: 1, et_weight: 0.8, knn_weight: 0.1': 0.9587080124198556,\n",
       " 'lgbm_weight: 0.8, et_weight: 1, knn_weight: 0.1': 0.9586676882132344,\n",
       " 'lgbm_weight: 1, et_weight: 1, knn_weight: 0.1': 0.9586273640066132,\n",
       " 'lgbm_weight: 0.8, et_weight: 0.8, knn_weight: 0.1': 0.9585467155933707,\n",
       " 'lgbm_weight: 0.8, et_weight: 0.8, knn_weight: 0.15': 0.9585467155933707,\n",
       " 'lgbm_weight: 0.8, et_weight: 1, knn_weight: 0.15': 0.9585467155933707,\n",
       " 'lgbm_weight: 1.2, et_weight: 1, knn_weight: 0.1': 0.9581031493205371,\n",
       " 'lgbm_weight: 1, et_weight: 1.2, knn_weight: 0.05': 0.9579821767006734,\n",
       " 'lgbm_weight: 1.2, et_weight: 1, knn_weight: 0.05': 0.9578208798741885,\n",
       " 'lgbm_weight: 1, et_weight: 1, knn_weight: 0.15': 0.9575789346344611,\n",
       " 'lgbm_weight: 1.2, et_weight: 0.8, knn_weight: 0.15': 0.9575789346344611,\n",
       " 'lgbm_weight: 1.2, et_weight: 0.8, knn_weight: 0.1': 0.9569337473285213,\n",
       " 'lgbm_weight: 1.2, et_weight: 0.8, knn_weight: 0.05': 0.955844993749748,\n",
       " 'lgbm_weight: 0.8, et_weight: 1.2, knn_weight: 0.05': 0.8684221137949111}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data = sorted(accuracy_list3.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_dict = {k: v for k, v in sorted_data}\n",
    " \n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.9600387112383564\n"
     ]
    }
   ],
   "source": [
    "lr_final = LogisticRegression(C=5, penalty='l2',multi_class='multinomial', solver='lbfgs', n_jobs=-1)\n",
    "\n",
    "lgbm_weight, et_weight, knn_weight = 1.2, 1.2, 0.15\n",
    "\n",
    "stack_final_X_train = np.concatenate((lgbm_train*lgbm_weight, et_train*et_weight, knn_train*knn_weight), axis=1)\n",
    "stack_final_X_test= np.concatenate((lgbm_test*lgbm_weight, et_test*et_weight, knn_test*knn_weight), axis=1)\n",
    "\n",
    "lr_final.fit(stack_final_X_train, y_train)\n",
    "stack_final = lr_final.predict(stack_final_X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, stack_final)\n",
    "print('정확도:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf = LGBMClassifier(n_estimators=300,n_jobs=-1, max_depth=25, min_child_samples=20, reg_alpha=0.2,  random_state=0)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=300, n_jobs=-1, max_depth=52, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_clf, X_train, y_train, X_test, 5, n_random_state=1)\n",
    "et_train, et_test = get_stacking_base_datasets(et_clf, X_train, y_train, X_test, 5, n_random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./train_dupcol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df2['target'] = label_encoder.fit_transform(df2['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df2.drop(['target', 'gcd'], axis=1)\n",
    "y_train = df2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test_transform.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop('gcd', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf = LGBMClassifier(n_estimators=2500,n_jobs=-1, max_depth=25, min_child_samples=20, reg_alpha=0.2,  random_state=0)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=2500, n_jobs=-1, max_depth=52, random_state=0)\n",
    "knn_clf = KNeighborsClassifier(metric='manhattan', n_jobs=-1, weights='distance', n_neighbors=2)\n",
    "lr_final = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf.fit(X_train, y_train)\n",
    "lgbm_train_pred = lgbm_clf.predict(X_train)\n",
    "lgbm_test_pred = lgbm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lgbm_train_pred.npy', lgbm_train_pred)\n",
    "np.save('lgbm_test_pred.npy', lgbm_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train_pred = np.load('lgbm_train_pred.npy')\n",
    "lgbm_test_pred = np.load('lgbm_test_pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf.fit(X_train, y_train)\n",
    "et_train_pred = et_clf.predict(X_train)\n",
    "et_test_pred = et_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('et_train_pred.npy', et_train_pred)\n",
    "np.save('et_test_pred.npy', et_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_train_pred = np.load('et_train_pred.npy')\n",
    "et_test_pred = np.load('et_test_pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train, y_train)\n",
    "knn_train_pred = knn_clf.predict(X_train)\n",
    "knn_test_pred = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('knn_train_pred.npy', knn_train_pred)\n",
    "np.save('knn_test_pred.npy', knn_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_train_pred = np.load('knn_train_pred.npy')\n",
    "knn_test_pred = np.load('knn_test_pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train_pred_reshaped = lgbm_train_pred.reshape(-1, 1)\n",
    "lgbm_test_pred_reshaped = lgbm_test_pred.reshape(-1, 1)\n",
    "lgbm_train_pred_float = lgbm_train_pred_reshaped.astype(float)\n",
    "lgbm_test_pred_float = lgbm_test_pred_reshaped.astype(float)\n",
    "\n",
    "\n",
    "knn_train_pred_reshaped = knn_train_pred.reshape(-1, 1)\n",
    "knn_test_pred_reshaped = knn_test_pred.reshape(-1, 1)\n",
    "knn_train_pred_float = knn_train_pred_reshaped.astype(float)\n",
    "knn_test_pred_float = knn_test_pred_reshaped.astype(float)\n",
    "\n",
    "\n",
    "et_train_pred_reshaped = et_train_pred.reshape(-1, 1)\n",
    "et_test_pred_reshaped = et_test_pred.reshape(-1, 1)\n",
    "et_train_pred_float = et_train_pred_reshaped.astype(float)\n",
    "et_test_pred_float = et_test_pred_reshaped.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_weight, et_weight, knn_weight = 1.2, 1.2, 0.15\n",
    "\n",
    "stack_final_X_train = np.concatenate((lgbm_train_pred_float*lgbm_weight, et_train_pred_float*et_weight, knn_train_pred_float*knn_weight), axis=1)\n",
    "stack_final_X_test = np.concatenate((lgbm_test_pred_float*lgbm_weight, et_test_pred_float*et_weight, knn_test_pred_float*knn_weight), axis=1)\n",
    "\n",
    "lr_final.fit(stack_final_X_train, y_train)\n",
    "\n",
    "stack_final = lr_final.predict(stack_final_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 2, ..., 0, 0, 9])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_stack_final = label_encoder.inverse_transform(stack_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id = list(range(200000, 300000))\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_result = pd.DataFrame({'row_id': row_id, 'target': decoded_stack_final.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(n_estimators=600, n_jobs=-1,max_depth=52)\n",
    "et_clf.fit(X_train, y_train)\n",
    "et_pred = et_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Escherichia_fergusonii', 'Salmonella_enterica',\n",
       "       'Enterococcus_hirae', ..., 'Bacteroides_fragilis',\n",
       "       'Bacteroides_fragilis', 'Streptococcus_pyogenes'], dtype=object)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id = list(range(200000, 300000))\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_result = pd.DataFrame({'row_id': row_id, 'target': et_pred.tolist()})\n",
    "df_result.to_csv('result.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종모델 선택\n",
    "- ExtraTreesClassifier\n",
    "- LGBMClassifier\n",
    "- KNeighborsClassifier\n",
    "위의 모델들을 하위모델로 가지고\n",
    "\n",
    "\n",
    "메타모델로 LogisticRegression을 가지는 stacking모델을 구성\n",
    "\n",
    "최종 정확도는 96%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 향후계획 및 맺음말\n",
    "\n",
    "- 적용하지 못한 특성공학들을 적용하여 더 정확한 결과를 얻기\n",
    "    * EMD, 사용된 플라스미드의 크기\n",
    "- 더 다양한 알고리즘 태스트해보기\n",
    "- 딥러닝에서 적용가능한 모델이 있는지 확인 후 사용해보기\n",
    "\n",
    "- 데이터 분석을 통해서 많은 내용을 찾아냈지만 적용하지 못해서 너무 아쉬웠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감사합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Mashine_Learning'",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
